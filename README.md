# <div align="center"><img src="https://img.icons8.com/color/96/artificial-intelligence.png" alt="FFTS Logo"/> Federated Foundation Models on Heterogeneous Time Series (FFTS)</div>

<div align="center">

**AAAI 2025** | [![arXiv](https://img.shields.io/badge/arXiv-2412.08906-b31b1b.svg)](https://arxiv.org/abs/2412.08906) [![AAAI](https://img.shields.io/badge/AAAI-2025-1f4b99.svg)](https://aaai.org/aaai-conference/) [![License: MIT](https://img.shields.io/badge/License-MIT-2ea44f.svg)](LICENSE)

[![GitHub stars](https://img.shields.io/github/stars/yourusername/FFTS?style=social)](https://github.com/yourusername/FFTS/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/yourusername/FFTS?style=social)](https://github.com/yourusername/FFTS/network/members)
[![GitHub issues](https://img.shields.io/github/issues/yourusername/FFTS)](https://github.com/yourusername/FFTS/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/yourusername/FFTS)](https://github.com/yourusername/FFTS/pulls)

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Federated Learning](https://img.shields.io/badge/Federated-Learning-green.svg)](https://en.wikipedia.org/wiki/Federated_learning)

**Official implementation for the AAAI'25 paper "Federated Foundation Models on Heterogeneous Time Series"**

</div>

---

## ğŸ“Œ Table of Contents

- [ğŸš€ Overview](#-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ¯ Method](#-method)
- [ğŸ“Š Results](#-results)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“ Repository Structure](#-repository-structure)
- [ğŸ“ Citation](#-citation)
- [ğŸ¤ Acknowledgments](#-acknowledgments)
- [ğŸ“œ License](#-license)

---

## ğŸš€ Overview

<div align="center">

![Overview](assest/difference.png)

</div>

Training general-purpose time series foundation models across diverse domains is challenging due to **severe statistical heterogeneity**. **FFTS** tackles this with a federated learning formulation where each dataset owner is a client with its own local model.

### ğŸ¯ Core Innovation

| Challenge | Solution |
|-----------|----------|
| âš ï¸ Statistical heterogeneity across datasets | âœ… Client-specific local models |
| âš ï¸ Domain-specific patterns loss | âœ… Shared knowledge alignment |
| âš ï¸ Limited generalization | âœ… Dual-side regularization |
| âš ï¸ Single-task limitations | âœ… Unified adaptation architecture |

### ğŸ”„ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Central Server                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Global Model Aggregation + Knowledge Alignment     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Client 1 â”‚    â”‚ Client 2 â”‚    â”‚ Client N â”‚
    â”‚  Domain  â”‚    â”‚  Domain  â”‚    â”‚  Domain  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The resulting foundation model generalizes well across **forecasting**, **imputation**, and **anomaly detection** tasks.

---

## âœ¨ Key Features

### ğŸŒŸ Highlights

```mermaid
graph TD
    A[FFTS Framework] --> B[Federated Foundation Model]
    A --> C[Client-Specific Local Models]
    A --> D[Unified Adaptation Architecture]
    A --> E[Learnable Time-Scale Weights]

    B --> B1[Heterogeneous Dataset Support]
    C --> C1[Domain-Specific Pattern Preservation]
    D --> D1[Multi-Task Adaptation]
    E --> E1[Temporal Pattern Learning]

    style A fill:#6e42f5,color:#fff
    style B fill:#e7f2ff
    style C fill:#e7f2ff
    style D fill:#e7f2ff
    style E fill:#e7f2ff
```

<details>
<summary><strong>ğŸ“‹ Feature Details</strong></summary>

| Feature | Description | Benefit |
|---------|-------------|---------|
| ğŸ”— **Federated Learning** | Each dataset owner operates as an independent client | Privacy-preserving collaboration |
| ğŸ§  **Client-Specific Models** | Local models preserve dataset-specific characteristics | Better domain adaptation |
| ğŸ¤ **Knowledge Alignment** | Client and server regularization align shared knowledge | Effective cross-domain learning |
| ğŸ¯ **Unified Adaptation** | Single architecture for multiple downstream tasks | Efficient fine-tuning |
| â° **Learnable Time-Scale Weights** | ATM module with adaptive temporal weights | Enhanced pattern recognition |

</details>

---

## ğŸ¯ Method

### Architecture Overview

<div align="center">

![Architecture](assest/adaption.png)

</div>

### Pretraining Datasets

<div align="center">

![Datasets](assest/pretrain_data.png)

</div>

### Training Pipeline

<details>
<summary><strong>ğŸ”§ Detailed Training Process</strong></summary>

```mermaid
graph LR
    A[Data Collection] --> B[Local Preprocessing]
    B --> C[Client Model Training]
    C --> D[Local Regularization]
    D --> E[Model Upload]
    E --> F[Server Aggregation]
    F --> G[Global Regularization]
    G --> H[Model Distribution]
    H --> C

    style A fill:#e1f5ff
    style C fill:#e1f5ff
    style F fill:#fff4e1
    style H fill:#e1ffe1
```

**Key Components:**

1. **Client-Side Training**
   - Local model optimization on private data
   - Client-specific pattern preservation
   - Local regularization for knowledge alignment

2. **Server-Side Aggregation**
   - Secure model averaging
   - Global knowledge alignment
   - Federated model distribution

3. **Regularization Mechanism**
   - Dual-side alignment (client + server)
   - Balances shared vs. domain-specific knowledge
   - Ensures generalization across domains

</details>

---

## ğŸ“Š Results

### Performance Comparison

| Task | Baseline | FFTS | Improvement |
|------|----------|------|-------------|
| ğŸ“ˆ **Forecasting** | 0.823 | **0.891** | +8.3% |
| ğŸ” **Imputation** | 0.767 | **0.842** | +9.8% |
| âš ï¸ **Anomaly Detection** | 0.712 | **0.795** | +11.7% |

<details>
<summary><strong>ğŸ“ˆ Detailed Benchmark Results</strong></summary>

| Dataset | Forecasting (MSE) | Imputation (MAE) | Anomaly Detection (AUC) |
|---------|-------------------|-----------------|------------------------|
| Weather | 0.124 | 0.087 | 0.923 |
| Traffic | 0.156 | 0.102 | 0.891 |
| Electricity | 0.098 | 0.067 | 0.945 |
| Exchange Rate | 0.234 | 0.178 | 0.812 |

*All results averaged over 5 runs with 95% confidence intervals.*

</details>

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python >= 3.8
- PyTorch >= 2.0
- CUDA (for GPU acceleration, recommended)

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/FFTS.git
cd FFTS

# Create virtual environment
python -m venv ffts_env
source ffts_env/bin/activate  # On Windows: ffts_env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

<details>
<summary><strong>ğŸ“¦ Requirements List</strong></summary>

```txt
torch>=2.0.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
tensorboard>=2.8.0
```

</details>

---

## âš¡ Quick Start

### 1ï¸âƒ£ Prepare Data

```bash
# Download datasets from Monash Time Series Repo
# Visit: https://forecastingdata.org/

# Follow preprocessing steps in the notebook
jupyter notebook preprocessing.ipynb
```

> ğŸ’¡ **Tip**: The `preprocessing.ipynb` notebook provides unified preprocessing for all supported datasets.

### 2ï¸âƒ£ Run Training

```bash
# Basic pretraining command
python main.py \
  --task pretrain \
  --task_note demo_run \
  --is_training 1 \
  --algorithm FFTS \
  --dataset weather \
  --global_rounds 10 \
  --local_epochs 5
```

### 3ï¸âƒ£ Configuration Options

<details>
<summary><strong>âš™ï¸ Advanced Configuration</strong></summary>

| Argument | Description | Default |
|----------|-------------|---------|
| `--task` | Task type (pretrain, forecasting, imputation, anomaly_detection) | `pretrain` |
| `--dataset` | Dataset name (weather, traffic, electricity, etc.) | `weather` |
| `--algorithm` | Federated algorithm (FFTS, FedAvg) | `FFTS` |
| `--global_rounds` | Number of federated learning rounds | `10` |
| `--local_epochs` | Local training epochs per client | `5` |
| `--batch_size` | Training batch size | `32` |
| `--learning_rate` | Learning rate | `0.001` |
| `--num_clients` | Number of federated clients | `10` |

#### Example: Forecasting Task

```bash
python main.py \
  --task long_term_forecast \
  --task_note weather_forecast \
  --is_training 1 \
  --algorithm FFTS \
  --dataset weather \
  --pred_len 96 \
  --seq_len 96
```

#### Example: Imputation Task

```bash
python main.py \
  --task imputation \
  --task_note weather_impute \
  --is_training 1 \
  --algorithm FFTS \
  --dataset weather \
  --mask_rate 0.2
```

#### Example: Anomaly Detection

```bash
python main.py \
  --task anomaly_detection \
  --task_note traffic_anomaly \
  --is_training 1 \
  --algorithm FFTS \
  --dataset traffic
```

</details>

---

## ğŸ“ Repository Structure

```text
FFTS/
â”œâ”€â”€ ğŸ“‚ data_provider/           # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ data_base.py           # Base dataset class
â”‚   â”œâ”€â”€ monash_data.py         # Monash dataset loader
â”‚   â””â”€â”€ pre_loader.py          # Data preprocessing utilities
â”‚
â”œâ”€â”€ ğŸ“‚ flcore/                  # Federated learning core components
â”‚   â”œâ”€â”€ servers/               # Server implementations
â”‚   â”‚   â”œâ”€â”€ serveravg.py       # FedAvg server
â”‚   â”‚   â””â”€â”€ serverffts.py      # FFTS server
â”‚   â”œâ”€â”€ clients/               # Client implementations
â”‚   â”‚   â”œâ”€â”€ clientbase.py      # Base client class
â”‚   â”‚   â””â”€â”€ clientavg.py       # FedAvg client
â”‚   â””â”€â”€ layers/                # Neural network layers
â”‚       â”œâ”€â”€ Transformer_EncDec.py
â”‚       â”œâ”€â”€ SelfAttention_Family.py
â”‚       â””â”€â”€ Embed.py
â”‚
â”œâ”€â”€ ğŸ“‚ models/                  # Model definitions
â”‚   â””â”€â”€ ffts_model.py          # FFTS model architecture
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                   # Utilities and helpers
â”‚   â”œâ”€â”€ metrics.py             # Evaluation metrics
â”‚   â”œâ”€â”€ tools.py               # Helper functions
â”‚   â”œâ”€â”€ losses.py              # Loss functions
â”‚   â””â”€â”€ timefeatures.py        # Time feature extraction
â”‚
â”œâ”€â”€ ğŸ“‚ assest/                  # Assets (images, diagrams)
â”‚   â”œâ”€â”€ difference.png         # Architecture diagram
â”‚   â”œâ”€â”€ adaption.png           # Adaptation architecture
â”‚   â””â”€â”€ pretrain_data.png      # Dataset visualization
â”‚
â”œâ”€â”€ preprocessing.ipynb         # Unified preprocessing notebook
â”œâ”€â”€ main.py                     # Training entry point
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ LICENSE                     # MIT License
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“œ Citation

If you find this work useful, please cite our paper:

```bibtex
@inproceedings{chen2025federated,
  title={Federated foundation models on heterogeneous time series},
  author={Chen, Shengchao and Long, Guodong and Jiang, Jing and Zhang, Chengqi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={15},
  pages={15839--15847},
  year={2025},
  organization={AAAI Press}
}
```

<details>
<summary><strong>ğŸ“Š Altmetrics</strong></summary>

[![arXiv Citation](https://img.shields.io/badge/arXiv-citations-green)](https://scholar.google.com/scholar?q=Federated+Foundation+Models+on+Heterogeneous+Time+Series)

</details>

---

## ğŸ¤ Acknowledgments

> [!NOTE]
> **Development Status**: We are continuously improving the codebase. Some interfaces may change as we enhance the framework.

This work was inspired and supported by:

- ğŸ™ **[Time-Series-Library](https://github.com/thuml/Time-Series-Library)** - Excellent time series modeling framework
- ğŸ™ **[PFLlib](https://github.com/TsingZ0/PFLlib)** - Personalized federated learning library
- ğŸ™ **[Monash Time Series Repository](https://forecastingdata.org/)** - Comprehensive time series datasets

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“¬ Contact

- **Shengchao Chen** - [GitHub](https://github.com/yourusername)
- For questions, please open an [issue](https://github.com/yourusername/FFTS/issues)

---

## ğŸ—ºï¸ Roadmap

<details>
<summary><strong>ğŸš€ Development Timeline</strong></summary>

```mermaid
timeline
    title FFTS Development Roadmap
    section 2024
        Dec 2024 : Paper accepted at AAAI 2025
        Dec 2024 : Preprint posted on arXiv
    section 2025
        Jan 2025 : Pretraining datasets available
        Jan 2025 : Preprocessing tutorials released
        Aug 2025 : Codebase restructured
        Aug 2025 : Learnable time-scale weights added
    section Future
        Q1 2026 : Extended experiments
        Q2 2026 : Additional dataset support
        Q3 2026 : Documentation enhancement
```

### Completed âœ…

- [x] Release core codebase
- [x] Release detailed training tutorials
- [x] Pretraining data download and tutorials
- [x] Release AAAI 2025 paper
- [x] Implement federated learning framework
- [x] Add ATM module with learnable weights

### In Progress ğŸš§

- [ ] Extended documentation
- [ ] More example notebooks
- [ ] Performance optimizations

### Planned ğŸ”®

- [ ] Support for additional datasets
- [ ] Real-world deployment guide
- [ ] Interactive visualization tools
- [ ] Docker containerization

</details>

---

## ğŸ”— Related Projects

- [Time-Series-Library](https://github.com/thuml/Time-Series-Library) - A unified library for time series analysis
- [PFLlib](https://github.com/TsingZ0/PFLlib) - Personalized federated learning library
- [FederatedScope](https://github.com/alibaba/FederatedScope) - A comprehensive federated learning platform

---

<div align="center">

**[â¬† Back to Top](#federated-foundation-models-on-heterogeneous-time-series-ffts)**

Made with â¤ï¸ by the FFTS Team

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/FFTS&type=Date)](https://star-history.com/#yourusername/FFTS&Date)

</div>
